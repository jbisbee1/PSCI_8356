
\documentclass[11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[abbr]{harvard}
\usepackage{amssymb}
\usepackage{setspace,graphics,epsfig,amsmath,rotating,amsfonts,mathpazo}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=BibTeX}
%TCIDATA{Created=Sunday, September 21, 2008 09:09:16}
%TCIDATA{LastRevised=Monday, November 11, 2013 08:24:44}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Articles\SW\article.egan">}

\parskip=0pt
\topmargin=0 in \headheight=0in \headsep=0in \topskip=0in \textheight=9in \oddsidemargin=0in \evensidemargin=0in \textwidth=6.5in
\input{tcilatex}
\setcounter{section}{11}

\begin{document}


\singlespacing

\textbf{NYU Department of Politics - Quant I}

\textbf{Fall 2013 - Prof.\ Patrick Egan}

\doublespacing

\section{Lecture 12}

\subsection{Associations between and among variables}

\begin{itemize}
\item Until now, we've focused on description and inference regarding one
variable, and at times we've considered description and inference regarding
comparisons of two variables drawn from different units.

\item Now we turn the page to a task that political scientists spend a lot
of time doing: considering relationships (or associations) between variables
drawn from the \textit{same }units.

\item We'll start by considering the relationship between two variables, but
quickly move on to considering relationships among many variables.

\item First some very familiar terminology:

\begin{itemize}
\item When we talk about a bivariate relationship, we typically refer to the
two variables as $X$ and $Y.$ \ 

\item We, of course, usually choose these labels with a causal model in
mind: specifically, that $X$ causes $Y$. \ But that assumption is
UNNECESSARY for what we'll be discussing today. \ In this vein, $X$ is known
as the \textquotedblleft independent variable,\textquotedblright\ and $Y$ is
known as the \textquotedblleft dependent variable.\textquotedblright
\end{itemize}

\item We'll proceed in several steps (list on board):

\begin{enumerate}
\item DISPLAYING the relationship

\begin{enumerate}
\item Cross(tabulations)

\item Scatterplot

\item Boxplot

\item "Binning out" X
\end{enumerate}

\item SUMMARIZING\ the relationship NONPARAMETRICALLY with central
tendencies of Y by values of X:

\begin{enumerate}
\item TABLE: Summary statistics of Y for values of X

\item FIGURES (generally appropriate when X, Y or both are interval-level or
higher)

\begin{enumerate}
\item bar chart (more values of X, Y is interval-level)

\item scatterplot with smoother, indicating the central tendency of Y by
values of X
\end{enumerate}
\end{enumerate}

\item SUMMARIZING\ the relationship PARAMETRICALLY, that is saying how
closely the relationship approximates a perfectly linear relationship

\begin{enumerate}
\item Correlation

\item Bivariate Regression
\end{enumerate}

\item Making INFERENCES about the nature of the relationship in a population
from the relationship in a sample

\begin{enumerate}
\item Non-parametric: Pearson's chi-squared

\item Parametric: Correlation

\item Parametric: Linear regression
\end{enumerate}
\end{enumerate}

\item (Walk through Parts 1 and 2 with handout)

\item (Before part 3) To simplify this task, we will often need to resort to 
\textit{models }that describe the theoretical relationship between the
variables. \ As usual, we face a tradeoff between parsimony and precision. \
What these models buy us is parsimony: the assumptions we make with models
allow us to summarize relationships between and among variables with just a
few numbers. \ But what we pay for with models is that we lose some detail
about the world. \ And if our theoretical model is incorrect, our
descriptions, inferences and predictions about relationships between and
among variables will be wrong. \ 

\item It's worth noting that (so far) our development of statistical tools
for making inferences about univariate data has been remarkably free of
assumptions. \ In fact, we can develop a list of these assumptions--and it's
a short list:

\begin{itemize}
\item in making inferences about the population mean, $\mu ,$ with \textit{%
large} samples,

\begin{itemize}
\item \textbf{identicality} is necessary for our estimator, $\overline{Y},$
to be an unbiased estimator of $\mu .$

\item \textbf{independence }is necessary in order for us to say that the
variance of $\overline{Y}$ is equal to $\frac{\sigma ^{2}}{n}.$

\begin{itemize}
\item both of these assumptions are met when we have a \textbf{random sample}%
.
\end{itemize}
\end{itemize}

\item in making inferences about the population mean, $\mu ,$ with \textit{%
small} samples, we need an additional assumption:

\begin{itemize}
\item the distribution of the underlying population is \textbf{Normal}.

\begin{itemize}
\item although the tools we've learned are robust under moderate departures
from this assumption.
\end{itemize}
\end{itemize}

\item finally, in making inferences about the differences between two
population means, we need additional assumptions:

\begin{itemize}
\item in large samples:

\begin{itemize}
\item the two samples are drawn \textbf{independently}
\end{itemize}

\item in small samples:

\begin{itemize}
\item the two samples are drawn \textbf{independently}, they have the same
variance, and the underlying populations are Normal (although, again, these
tools are robust under moderate departures from this last assumption).
\end{itemize}
\end{itemize}
\end{itemize}

\item Of course, there are lots of instances when these assumptions don't
hold, and statisticians spend a lot of time thinking about how to revise
their tools to account for these cases. \ But still, it's a remarkably short
list. \ It will get a lot longer as we move to bivariate and multivariate
analysis.

\item To begin thinking together about parametric ways to describe a
bivariate relationship, let's revisit the notion of correlation. \ You'll
recall the population correlation coefficient, $\rho ,$ which is equal to%
\begin{equation*}
\rho =\frac{COV(Y_{1},Y_{2})}{\sigma _{1}\sigma _{2}}=\frac{E\left[
(Y_{1}-\mu _{1})(Y_{2}-\mu _{2})\right] }{\sigma _{1}\sigma _{2}}.
\end{equation*}

\item $\rho $ is, of course, is a theoretical quantity. \ Like $\mu $ or $%
\sigma ^{2},$ we never actually observe it. \ But the maximum-likelihood
estimator of $\rho $ is the sample correlation coefficient, $r:$%
\begin{equation*}
r=\frac{\sum\nolimits_{i}\left( X_{i}-\overline{X}\right) \left( Y_{i}-%
\overline{Y}\right) }{\sqrt{\sum\nolimits_{i}\left( X_{i}-\overline{X}%
\right) ^{2}\sum\nolimits_{i}\left( Y_{i}-\overline{Y}\right) ^{2}}}.
\end{equation*}

\item Because the quantities that appear in $r$ are used often in regression
analysis, they have special symbols: 
\begin{eqnarray*}
S_{xy} &=&\sum\nolimits_{i}\left( X_{i}-\overline{X}\right) \left( Y_{i}-%
\overline{Y}\right) \\
S_{xx} &=&\sum\nolimits_{i}\left( X_{i}-\overline{X}\right)
^{2}=\sum\nolimits_{i}\left( X_{i}-\overline{X}\right) \left( X_{i}-%
\overline{X}\right) \\
S_{yy} &=&\sum\nolimits_{i}\left( Y_{i}-\overline{Y}\right) ^{2}
\end{eqnarray*}%
So how can we write $r?$%
\begin{equation*}
r=\frac{\sum\nolimits_{i}\left( X_{i}-\overline{X}\right) \left( Y_{i}-%
\overline{Y}\right) }{\sqrt{\sum\nolimits_{i}\left( X_{i}-\overline{X}%
\right) ^{2}\sum\nolimits_{i}\left( Y_{i}-\overline{Y}\right) ^{2}}}=\frac{%
S_{xy}}{\sqrt{S_{xx}S_{yy}}}
\end{equation*}

\item now, (finally) linear regression.

\item what if we wanted to put some more meat on the extent to which a
relationship between $x$ and $y$ is linear? \ In particular, what if we
wanted to say something about the line that best represented the
relationship in linear terms? \ Well, we'd start by specifying a generic
formula for that line, which convention has led us to write as follows:%
\begin{equation*}
y=\beta _{0}+\beta _{1}x
\end{equation*}

where $\beta _{0}$ is the intercept and $\beta _{1}$ the slope.

\item Draw on board:

\item \FRAME{itbpF}{3.8771in}{2.3337in}{0in}{}{}{Figure}{\special{language
"Scientific Word";type "GRAPHIC";maintain-aspect-ratio TRUE;display
"USEDEF";valid_file "T";width 3.8771in;height 2.3337in;depth
0in;original-width 5.76in;original-height 3.4562in;cropleft "0";croptop
"1";cropright "1";cropbottom "0";tempfilename
'MVPFV503.wmf';tempfile-properties "XPR";}}

\item There are many such lines we might choose to represent this
relationship in linear terms. \ And even the best line can't hit every point
on the nose; it will certainly make mistakes. \ We define the mistake our
line makes for any particular observation $i$ as the observation $y_{i}$
minus the \textquotedblleft fitted value\textquotedblright\ for that
observation, which we'll write as $\widehat{y}_{i}:$

\item \FRAME{itbpF}{3.6106in}{2.1735in}{0in}{}{}{Figure}{\special{language
"Scientific Word";type "GRAPHIC";maintain-aspect-ratio TRUE;display
"USEDEF";valid_file "T";width 3.6106in;height 2.1735in;depth
0in;original-width 5.76in;original-height 3.4562in;cropleft "0";croptop
"1";cropright "1";cropbottom "0";tempfilename
'MVPFV504.wmf';tempfile-properties "XPR";}}

\item Another term for these mistakes is residuals. \ We for any observation 
$i$ we write the residual associated with that observation as $\widehat{u}%
_{i}=y_{i}-\widehat{y}_{i}.$

\item An obvious criterion to use for picking the best line would be to
minimize the mistakes it makes as it proceeds through the $xy$ plane. \ That
is, we could minimize $\left\vert y_{i}-\widehat{y}_{i}\right\vert $ as much
as possible by minimizing $\sum_{i}$ $\left\vert y_{i}-\widehat{y}%
_{i}\right\vert .$

\item Well (as you will or have already learned), the absolute value
function is a lousy one to work with mathematically. \ It has annoying
properties that do not lend itself to easy manipulation. \ A more useful and
easier sum to minimize is%
\begin{equation*}
SSR\equiv \sum_{i}\left( y_{i}-\widehat{y}_{i}\right) ^{2},
\end{equation*}

the \textbf{sum of squared residuals (SSR). \ }Note that the square of the
residual has the nice property of becoming bigger as the magnitude of the
residual increases. \ (It has the less felicitous property of counting
bigger distances as greater than smaller distances: for example if we double
a distance of 3 to 6, the corresponding squares are 9 and 36: a quadrupling.
\ Thus whatever method we pick that minimizes SSR is going to work harder to
minimize big deviations from the line than it probably should. \ But we're
getting ahead of ourselves.)

\item Now let's go back to our formula for a line. \ Let's do two things:

\begin{itemize}
\item rewrite it with hats to be clear that we are generating estimates
rather than saying anything specific (yet) about population values, and

\item add subscripts for x and y:%
\begin{equation*}
\widehat{y}_{i}=\widehat{\beta }_{0}+\widehat{\beta }_{1}x_{i}
\end{equation*}
\end{itemize}

\item Now let's consider the residual again, $\widehat{u}_{i},$ and
substitute:%
\begin{eqnarray*}
\widehat{u}_{i} &=&y_{i}-\widehat{y}_{i} \\
&=&y_{i}-\left( \widehat{\beta }_{0}+\widehat{\beta }_{1}x_{i}\right) \\
\left( \widehat{u}_{i}\right) ^{2} &=&\left[ y_{i}-\left( \widehat{\beta }%
_{0}+\widehat{\beta }_{1}x_{i}\right) \right] ^{2} \\
\sum\nolimits_{i}\left( \widehat{u}_{i}\right) ^{2} &=&\sum\nolimits_{i}%
\left[ y_{i}-\left( \widehat{\beta }_{0}+\widehat{\beta }_{1}x_{i}\right) %
\right] ^{2}
\end{eqnarray*}

\item So the $\widehat{\beta }_{0}$ and $\widehat{\beta }_{1}$ that minimize
this quantity are the intercept and the slope of what is known as the 
\textbf{least squares }line--the line that best represents the relationship
between x and y. \ We can use the tools of calculus to find $\widehat{\beta }%
_{0}$ and $\widehat{\beta }_{1}$ by taking the partial derivative of SSR
with respect to each of these quantities and setting these derivatives equal
to zero:%
\begin{eqnarray*}
\frac{\partial SSR}{\partial \widehat{\beta }_{0}} &=&\frac{\partial }{%
\partial \widehat{\beta }_{0}}\sum\nolimits_{i}\left[ y_{i}-\left( \widehat{%
\beta }_{0}+\widehat{\beta }_{1}x_{i}\right) \right] ^{2} \\
&=&-2\sum\nolimits_{i}y_{i}-\left( \widehat{\beta }_{0}+\widehat{\beta }%
_{1}x_{i}\right) \\
&=&-2\left( \sum\nolimits_{i}y_{i}-n\widehat{\beta }_{0}-\widehat{\beta }%
_{1}\sum\nolimits_{i}x_{i}\right) =0
\end{eqnarray*}%
\begin{eqnarray*}
\frac{\partial SSR}{\partial \widehat{\beta }_{1}} &=&\frac{\partial }{%
\partial \widehat{\beta }_{1}}\sum\nolimits_{i}\left[ y_{i}-\left( \widehat{%
\beta }_{0}+\widehat{\beta }_{1}x_{i}\right) \right] ^{2} \\
&=&-2\sum\nolimits_{i}\left[ y_{i}-\left( \widehat{\beta }_{0}+\widehat{%
\beta }_{1}x_{i}\right) \right] x_{i} \\
&=&-2\left( \sum\nolimits_{i}x_{i}y_{i}-\widehat{\beta }_{0}\sum%
\nolimits_{i}x_{i}-\widehat{\beta }_{1}\sum\nolimits_{i}x_{i}^{2}\right) =0
\end{eqnarray*}

\item These lead to the \textbf{normal equations}%
\begin{eqnarray*}
n\widehat{\beta }_{0}+\widehat{\beta }_{1}\sum\nolimits_{i}x_{i}
&=&\sum\nolimits_{i}y_{i} \\
\widehat{\beta }_{0}\sum\nolimits_{i}x_{i}+\widehat{\beta }%
_{1}\sum\nolimits_{i}x_{i}^{2} &=&\sum\nolimits_{i}x_{i}y_{i}
\end{eqnarray*}

\item Rewrite in matrix form:%
\begin{equation*}
\begin{bmatrix}
n & \sum\nolimits_{i}x_{i} \\ 
\sum\nolimits_{i}x_{i} & \sum\nolimits_{i}x_{i}^{2}%
\end{bmatrix}%
\begin{bmatrix}
\widehat{\beta }_{0} \\ 
\widehat{\beta }_{1}%
\end{bmatrix}%
=%
\begin{bmatrix}
\sum\nolimits_{i}y_{i} \\ 
\sum\nolimits_{i}x_{i}y_{i}%
\end{bmatrix}%
\end{equation*}

\item So:%
\begin{equation*}
\begin{bmatrix}
\widehat{\beta }_{0} \\ 
\widehat{\beta }_{1}%
\end{bmatrix}%
=%
\begin{bmatrix}
n & \sum\nolimits_{i}x_{i} \\ 
\sum\nolimits_{i}x_{i} & \sum\nolimits_{i}x_{i}^{2}%
\end{bmatrix}%
^{-1}%
\begin{bmatrix}
\sum\nolimits_{i}y_{i} \\ 
\sum\nolimits_{i}x_{i}y_{i}%
\end{bmatrix}%
\end{equation*}

\item The inverse of a 2x2 matrix $\mathbf{A=}%
\begin{bmatrix}
a & b \\ 
c & d%
\end{bmatrix}%
$ is $\mathbf{A}^{\mathbf{-1}}=\frac{1}{\det \mathbf{A}}%
\begin{bmatrix}
d & -b \\ 
-c & a%
\end{bmatrix}%
=\frac{1}{ad-bc}%
\begin{bmatrix}
d & -b \\ 
-c & a%
\end{bmatrix}%
$

\item So 
\begin{equation*}
\begin{bmatrix}
n & \sum\nolimits_{i}x_{i} \\ 
\sum\nolimits_{i}x_{i} & \sum\nolimits_{i}x_{i}^{2}%
\end{bmatrix}%
^{-1}=\frac{1}{n\sum\nolimits_{i}x_{i}^{2}-\left(
\sum\nolimits_{i}x_{i}\right) ^{2}}%
\begin{bmatrix}
\sum\nolimits_{i}x_{i}^{2} & -\sum\nolimits_{i}x_{i} \\ 
-\sum\nolimits_{i}x_{i} & n%
\end{bmatrix}%
\end{equation*}

\item And thus%
\begin{eqnarray*}
\begin{bmatrix}
\widehat{\beta }_{0} \\ 
\widehat{\beta }_{1}%
\end{bmatrix}
&=&\frac{1}{n\sum\nolimits_{i}x_{i}^{2}-\left( \sum\nolimits_{i}x_{i}\right)
^{2}}%
\begin{bmatrix}
\sum\nolimits_{i}x_{i}^{2} & -\sum\nolimits_{i}x_{i} \\ 
-\sum\nolimits_{i}x_{i} & n%
\end{bmatrix}%
\begin{bmatrix}
\sum\nolimits_{i}y_{i} \\ 
\sum\nolimits_{i}x_{i}y_{i}%
\end{bmatrix}%
,\text{ so} \\
\widehat{\beta }_{0} &=&\frac{\sum\nolimits_{i}x_{i}^{2}\sum%
\nolimits_{i}y_{i}-\sum\nolimits_{i}x_{i}\sum\nolimits_{i}x_{i}y_{i}}{%
n\sum\nolimits_{i}x_{i}^{2}-\left( \sum\nolimits_{i}x_{i}\right) ^{2}},%
\widehat{\beta }_{1}=\frac{n\sum\nolimits_{i}x_{i}y_{i}-\sum%
\nolimits_{i}x_{i}\sum\nolimits_{i}y_{i}}{n\sum\nolimits_{i}x_{i}^{2}-\left(
\sum\nolimits_{i}x_{i}\right) ^{2}}.
\end{eqnarray*}
\end{itemize}

\section{Lecture 13}

\subsection{Formulas for the slope and intercept of the bivariate regression
line}

\begin{itemize}
\item Last time we left off with 
\begin{eqnarray*}
\widehat{\beta }_{0} &=&\frac{\sum\nolimits_{i}x_{i}^{2}\sum%
\nolimits_{i}y_{i}-\sum\nolimits_{i}x_{i}\sum\nolimits_{i}x_{i}y_{i}}{%
n\sum\nolimits_{i}x_{i}^{2}-\left( \sum\nolimits_{i}x_{i}\right) ^{2}},%
\widehat{\beta }_{1}=\frac{n\sum\nolimits_{i}x_{i}y_{i}-\sum%
\nolimits_{i}x_{i}\sum\nolimits_{i}y_{i}}{n\sum\nolimits_{i}x_{i}^{2}-\left(
\sum\nolimits_{i}x_{i}\right) ^{2}}. \\
\text{We can simplify these as follows} &\text{:}& \\
\widehat{\beta }_{0} &=&\frac{n\overline{y}\sum\nolimits_{i}x_{i}^{2}-n%
\overline{x}\sum\nolimits_{i}x_{i}y_{i}}{n\sum\nolimits_{i}x_{i}^{2}-\left( n%
\overline{x}\right) ^{2}}=\frac{\overline{y}\sum\nolimits_{i}x_{i}^{2}-%
\overline{x}\sum\nolimits_{i}x_{i}y_{i}}{\sum\nolimits_{i}x_{i}^{2}-n\left( 
\overline{x}\right) ^{2}}\text{ \ and similarly} \\
\widehat{\beta }_{1} &=&\frac{n\sum\nolimits_{i}x_{i}y_{i}-n^{2}\overline{x}%
\overline{y}}{n\sum\nolimits_{i}x_{i}^{2}-n^{2}\left( \overline{x}\right)
^{2}}=\frac{\sum\nolimits_{i}x_{i}y_{i}-n\overline{x}\overline{y}}{%
\sum\nolimits_{i}x_{i}^{2}-n\left( \overline{x}\right) ^{2}}.
\end{eqnarray*}

\item Simplify further by substituting the symbols we learned earlier with a
little manipulation:%
\begin{eqnarray*}
S_{xx} &=&\sum\nolimits_{i}\left( X_{i}-\overline{X}\right)
^{2}=\sum\nolimits_{i}X_{i}^{2}+\sum\nolimits_{i}\overline{X}%
^{2}-\sum\nolimits_{i}2X_{i}\overline{X} \\
&=&\sum\nolimits_{i}X_{i}^{2}-n\overline{X}^{2}; \\
S_{yy} &=&\sum\nolimits_{i}Y_{i}^{2}-n\overline{Y}^{2} \\
S_{xy} &=&\sum\nolimits_{i}\left( X_{i}-\overline{X}\right) \left( Y_{i}-%
\overline{Y}\right) =\sum\nolimits_{i}X_{i}Y_{i}-\sum\nolimits_{i}X_{i}%
\overline{Y}-\sum\nolimits_{i}\overline{X}Y_{i}+\sum\nolimits_{i}\overline{X}%
\overline{Y} \\
&=&\sum\nolimits_{i}X_{i}Y_{i}-n\sum\nolimits_{i}\overline{X}\overline{Y}
\end{eqnarray*}

\item So:%
\begin{eqnarray*}
\widehat{\beta }_{1} &=&\frac{S_{xy}}{S_{xx}}.\text{ \ Note that because }%
\frac{cov(x,y)}{var(x)}=\frac{\frac{S_{xy}}{n}}{\frac{S_{xx}}{n}}, \\
\widehat{\beta }_{1} &=&\frac{cov(x,y)}{var(x)}.
\end{eqnarray*}

\item We can write $\widehat{\beta }_{0}$ more simply if note that we can
rewrite%
\begin{eqnarray*}
-2\left( \sum\nolimits_{i}y_{i}-n\widehat{\beta }_{0}-\widehat{\beta }%
_{1}\sum\nolimits_{i}x_{i}\right) &=&0 \\
\sum\nolimits_{i}y_{i}-n\widehat{\beta }_{0}-\widehat{\beta }%
_{1}\sum\nolimits_{i}x_{i} &=&0 \\
n\overline{y}-n\widehat{\beta }_{0}-n\widehat{\beta }_{1}\overline{x} &=&0 \\
\widehat{\beta }_{0} &=&\overline{y}-\widehat{\beta }_{1}\overline{x},\text{
and so} \\
\widehat{\beta }_{0} &=&\overline{y}-\frac{S_{xy}}{S_{xx}}\overline{x}.
\end{eqnarray*}

\item I will spare you a proof that we have satisfied the second-order
condition for a minimum!

\item Let's do a quick example. \ ("Handout on the Math of the Least-Squares
Line.")

\item Note a bit of terminology that gets thrown around:

\begin{itemize}
\item We \textquotedblleft regress $y$ on $x$\textquotedblright

\item $x$ is a \textquotedblleft regressor\textquotedblright

\item $x$ is on the "right hand side" of the regression equation
\end{itemize}

\item Note that we NOT talking about making inferences (yet). \ Everything
I've shown you thus far are simply mathematical properties that follow from
the desired criterion of fitting a least squares line to data. \ Although
this line tells us how well the data approximates a line, we have so far
made no assumptions about the distribution of $x$ and $y$ in the underlying
population. \ 

\item We will continue in this vein for a little while longer by discussing
additional mathematical properties of the least squares line. \ Here are a
few:

\begin{enumerate}
\item $\widehat{\beta }_{1}=\frac{\Delta \widehat{y}}{\Delta x}.$ \ If we
start with our fitted line, 
\begin{eqnarray*}
\widehat{y} &=&\widehat{\beta }_{0}+\widehat{\beta }_{1}x\,\ \text{\ and
take its derivative with respect to }x, \\
\frac{d\widehat{y}}{dx} &=&\widehat{\beta }_{1}\text{, we see (obviously)
that:} \\
&&\text{a one-unit change in }x\text{ is associated with a change of }%
\widehat{\beta }_{1}\text{ units of }\widehat{y}\text{\textbf{, }or} \\
\widehat{\beta }_{1} &=&\frac{\Delta \widehat{y}}{\Delta x}.
\end{eqnarray*}

Note that I'm saying associated with, not \textquotedblleft
causes.\textquotedblright\ \ Don't get too consumed by this yet; we'll talk
about this a lot more soon.

\item $\sum\nolimits_{i}\widehat{u}_{i}=0;$ $\overline{\widehat{u}}=0.$

\begin{itemize}
\item The sum of the residuals, $\sum\nolimits_{i}\widehat{u}_{i}$, equals
zero. \ We see this immediately by noting that%
\begin{eqnarray*}
\widehat{u}_{i} &=&y_{i}-\widehat{y}_{i}=y_{i}-\left( \widehat{\beta }_{0}+%
\widehat{\beta }_{1}x_{i}\right) , \\
\sum\nolimits_{i}\widehat{u}_{i} &=&\sum\nolimits_{i}y_{i}-\left( \widehat{%
\beta }_{0}+\widehat{\beta }_{1}x_{i}\right)
\end{eqnarray*}%
and noting that the formula for our regression line is the same as the
F.O.C. we found earlier associated with $\widehat{\beta }_{0}:$ 
\begin{eqnarray*}
\frac{\partial SSR}{\partial \widehat{\beta }_{0}} &=&\frac{\partial }{%
\partial \widehat{\beta }_{0}}\sum\nolimits_{i}\left[ y_{i}-\left( \widehat{%
\beta }_{0}+\widehat{\beta }_{1}x_{i}\right) \right] ^{2} \\
&=&-2\sum\nolimits_{i}y_{i}-\left( \widehat{\beta }_{0}+\widehat{\beta }%
_{1}x_{i}\right) =0 \\
&=&\sum\nolimits_{i}y_{i}-\left( \widehat{\beta }_{0}+\widehat{\beta }%
_{1}x_{i}\right) =0\text{, so} \\
\sum\nolimits_{i}\widehat{u}_{i} &=&0
\end{eqnarray*}

\item Note that this is just a property of the mechanics of fitting a line.
\ We say that $\sum\nolimits_{i}\widehat{u}_{i}=0$ \textquotedblleft by
construction.\textquotedblright\ \ This property is always the case, and it
tells us nothing about our data or the relationship between $x$ and $y$.

\item Note that this also means that 
\begin{eqnarray*}
\sum\nolimits_{i}\widehat{u}_{i} &=&n\overline{\widehat{u}}=0\text{, and so}
\\
\overline{\widehat{u}} &=&0.
\end{eqnarray*}

\item That is, the sample mean of the residuals is zero.
\end{itemize}

\item $cov(x,\widehat{u})=0.$

\begin{itemize}
\item By construction, the sample covariance between the regressors and the
residuals is zero. $\ $This follows from the F.O.C. associated with $%
\widehat{\beta }_{1}:$%
\begin{eqnarray*}
\frac{\partial SSR}{\partial \widehat{\beta }_{1}} &=&\frac{\partial }{%
\partial \widehat{\beta }_{1}}\sum\nolimits_{i}\left[ y_{i}-\left( \widehat{%
\beta }_{0}+\widehat{\beta }_{1}x_{i}\right) \right] ^{2} \\
&=&-2\sum\nolimits_{i}\left[ y_{i}-\left( \widehat{\beta }_{0}+\widehat{%
\beta }_{1}x_{i}\right) \right] x_{i}=0 \\
\sum\nolimits_{i}\left[ y_{i}-\left( \widehat{\beta }_{0}+\widehat{\beta }%
_{1}x_{i}\right) \right] x_{i} &=&0
\end{eqnarray*}

\item \ But since%
\begin{equation*}
y_{i}-\left( \widehat{\beta }_{0}+\widehat{\beta }_{1}x_{i}\right) =\widehat{%
u}_{i},
\end{equation*}%
We can substitute and write%
\begin{equation*}
\sum\nolimits_{i}\widehat{u}_{i}x_{i}=0.
\end{equation*}

\item Since the sample covariance of $x$ and $\widehat{u}$ is%
\begin{eqnarray*}
cov(x,\widehat{u}) &=&\frac{\sum\nolimits_{i}\left( \widehat{u}_{i}-%
\overline{\widehat{u}}\right) \left( x_{i}-\overline{x}\right) }{n},\text{
it follows that} \\
&=&\frac{\sum\nolimits_{i}\left( \widehat{u}_{i}\right) \left( x_{i}-%
\overline{x}\right) }{n} \\
&=&\frac{\sum\nolimits_{i}\widehat{u}_{i}x_{i}}{n}-\frac{\sum\nolimits_{i}%
\widehat{u}_{i}\overline{x}}{n} \\
&=&0-\frac{\overline{x}\sum\nolimits_{i}\widehat{u}_{i}}{n}=0-0=0.
\end{eqnarray*}
\end{itemize}

\item $cov\left( \widehat{y}_{i},\widehat{u}_{i}\right) =0.$ \ [LEAVE AS
EXERCISE.]

\begin{itemize}
\item The sample covariance between the fitted values and the residuals is
zero.

\begin{eqnarray*}
cov\left( \widehat{y}_{i},\widehat{u}_{i}\right) &=&\frac{%
\sum\nolimits_{i}\left( y_{i}-\overline{y}\right) \left( \widehat{u}_{i}-%
\overline{\widehat{u}}\right) }{n} \\
&=&\frac{\sum\nolimits_{i}\left( y_{i}-\overline{y}\right) \left( \widehat{u}%
_{i}\right) }{n} \\
&=&\frac{\sum\nolimits_{i}\widehat{u}_{i}y_{i}}{n}-\frac{\sum\nolimits_{i}%
\widehat{u}_{i}\overline{y}}{n} \\
&=&\frac{\sum\nolimits_{i}\widehat{u}_{i}y_{i}}{n}-\frac{\overline{y}%
\sum\nolimits_{i}\widehat{u}_{i}}{n}
\end{eqnarray*}

\item What is $\sum\nolimits_{i}\widehat{u}_{i}y_{i}$? \ It's%
\begin{equation*}
\sum\nolimits_{i}\widehat{u}_{i}y_{i}=\sum\nolimits_{i}\left( y_{i}-\widehat{%
y}_{i}\right) y_{i}
\end{equation*}
\end{itemize}

\item The point $(\overline{x},\overline{y})$ is always on the regression
line.

\begin{itemize}
\item Show this by substituting $\overline{x}$ for $x$, and $\widehat{\beta }%
_{0}=\overline{y}-\widehat{\beta }_{1}\overline{x}$ in the formula for the
regression line:%
\begin{eqnarray*}
\widehat{y} &=&\widehat{\beta }_{0}+\widehat{\beta }_{1}x \\
\widehat{y}\left( \overline{x}\right) &=&\overline{y}-\widehat{\beta }_{1}%
\overline{x}+\widehat{\beta }_{1}\overline{x} \\
\widehat{y}\left( \overline{x}\right) &=&\overline{y}.
\end{eqnarray*}
\end{itemize}

\item $\frac{\sum\nolimits_{i}\widehat{y_{i}}}{n}=\frac{\sum%
\nolimits_{i}y_{i}}{n}.$

\begin{itemize}
\item By construction, the sample average of the fitted values is equal to
the sample average of the observed $y$'s, that is $\frac{\sum\nolimits_{i}%
\widehat{y_{i}}}{n}=\frac{\sum\nolimits_{i}y_{i}}{n}.$ \ To show this, note
that%
\begin{eqnarray*}
y_{i} &=&\widehat{y_{i}}+\widehat{u_{i}}\text{, and so} \\
\sum\nolimits_{i}y_{i} &=&\sum\nolimits_{i}\widehat{y_{i}}+\sum\nolimits_{i}%
\widehat{u_{i}} \\
\sum\nolimits_{i}y_{i} &=&\sum\nolimits_{i}\widehat{y_{i}}+0 \\
\frac{\sum\nolimits_{i}y_{i}}{n} &=&\frac{\sum\nolimits_{i}\widehat{y_{i}}}{n%
}.
\end{eqnarray*}
\end{itemize}
\end{enumerate}

\item Another way to view the process of fitting a least squares line is to
think of it as decomposing each $y_{i}\ $into two parts: $\widehat{y_{i}}$
and $\widehat{u_{i}}.$ Of course\ $\widehat{u_{i}}\equiv y_{i}-\widehat{y}%
_{i}$ and so $y_{i}=\widehat{y}_{i}+\widehat{u_{i}}.$As noted earlier, $%
\widehat{y_{i}}$ and $\widehat{u_{i}}$ are by construction uncorrelated in
the sample.

\begin{itemize}
\item Define the \textbf{total sum of squares} (SST) as%
\begin{equation*}
SST\equiv \sum\nolimits_{i}\left( y_{i}-\overline{y}\right) ^{2}.
\end{equation*}

\item Define the \textbf{explained sum of squares} (SSE) as%
\begin{equation*}
SSE\equiv \sum\nolimits_{i}\left( \widehat{y}_{i}-\overline{y}\right) ^{2}
\end{equation*}

\item Recall that SSR is%
\begin{equation*}
SSR\equiv \sum_{i}\left( y_{i}-\widehat{y}_{i}\right) ^{2}\equiv
\sum\nolimits_{i}\widehat{u}_{i}^{2}
\end{equation*}
\end{itemize}

\item Let's first show that $SST=SSE+SSR.$ \ (From proof on p. 39 of
Wooldridge, but it is complete here.)

\item Start with the identity%
\begin{eqnarray*}
SST &=&\sum\nolimits_{i}\left( y_{i}-\overline{y}\right)
^{2}=\sum\nolimits_{i}\left( y_{i}-\widehat{y}_{i}+\widehat{y}_{i}-\overline{%
y}\right) ^{2} \\
&=&\sum\nolimits_{i}\left( \widehat{u}_{i}+\widehat{y}_{i}-\overline{y}%
\right) ^{2} \\
&=&\sum\nolimits_{i}\left( \widehat{u}_{i}\right)
^{2}+\sum\nolimits_{i}\left( \widehat{y}_{i}-\overline{y}\right)
^{2}+2\sum\nolimits_{i}\widehat{u}_{i}\left( \widehat{y}_{i}-\overline{y}%
\right) \\
&=&SSR+SSE+2\sum\nolimits_{i}\left( \widehat{y}_{i}-\overline{y}\right) 
\widehat{u}_{i}
\end{eqnarray*}

\item Since%
\begin{eqnarray*}
0 &=&cov\left( \widehat{y}_{i},\widehat{u}_{i}\right) \\
0 &=&\frac{\sum\nolimits_{i}\left( y_{i}-\overline{y}\right) \left( \widehat{%
u}_{i}-\overline{\widehat{u}}\right) }{n} \\
0 &=&\frac{\sum\nolimits_{i}\left( y_{i}-\overline{y}\right) \left( \widehat{%
u}_{i}\right) }{n} \\
0 &=&\sum\nolimits_{i}\left( y_{i}-\overline{y}\right) \left( \widehat{u}%
_{i}\right) ,
\end{eqnarray*}%
we can write%
\begin{equation*}
SST=SSR+SSE.
\end{equation*}
\end{itemize}

\subsection{More on Sums of Squares}

\begin{itemize}
\item Let's think about $SST,SSE\ $\ and $SSR$ in a bit more detail: \ 

\begin{itemize}
\item What is SST? \ It summarizes is the extent to which $Y$ differs from
its mean.

\item What is SSE? \ \ It summarizes the extent to which the predicted $Y$
differ from the mean of $Y$.

\item A line that perfectly fit all $x,y$ points would have the property
that $\widehat{y}_{i}=y_{i},$ making the ratio $\frac{SSE}{SST}=\frac{%
\sum\nolimits_{i}\left( \widehat{y}_{i}-\overline{y}\right) ^{2}}{%
\sum\nolimits_{i}\left( y_{i}-\overline{y}\right) ^{2}}=1.$

\item This is never the case, but we can exploit this fact to construct a
measure of the \textquotedblleft goodness of fit\textquotedblright\ of the
regression line to the data. \ Call the ratio $\frac{SSE}{SST}=R^{2}.$ \ 

\begin{itemize}
\item $R^{2}$ has a very intuitive interpretation: it is the proportion of
the sample variation in $y$ that is explained by $x$. \ It always ranges
between zero and one.

\item When reporting $R^{2}$, we typically report it to two decimal places.

\item Also note that $R^{2}=\frac{SSE}{SST}=1-\frac{SSR}{SST}.$

\item Where did $R^{2}$ get its name? \ Because it is also the case that in
the bivariate context, $R^{2}$ is the square of the correlation coefficient $%
r$: $R^{2}=\left( r\right) ^{2}.$

\begin{itemize}
\item You'll show that in an exercise on your homework.
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Least-Squares Regression is Invariant to Change in Units of
Measurement}

\begin{itemize}
\item A few other properties, proofs of which will also be on your homework:

\item What happens when we change the units in which the IV is measured
(typically by multiplying these values by some constant, $c$)?

\begin{itemize}
\item Recall that $\widehat{\beta }_{1}=$ the change in $\widehat{y}$
associated with a one-unit change in $x$.

\item So the change in $\widehat{y}$ associated with a one-unit change in $%
cx $ should be $\frac{\widehat{\beta }_{1}}{c}.$ \ And indeed it is:
\end{itemize}

\item So when the IV is multiplied by $c$,

\begin{itemize}
\item $\widehat{\beta }_{1}$ is divided by $c$.

\item $\widehat{\beta }_{0}$ does not change (it is the y-intercept, and $%
cx=0$ when $x=0)$

\item $R^{2}$ does not change.
\end{itemize}

\item When the DV is multiplied by $c$,

\begin{itemize}
\item (Again) recall that $\widehat{\beta }_{1}=$ the change in $\widehat{y}$
associated with a one-unit change in $x$.

\item So the change in $c\widehat{y}$ associated with a one-unit change in $%
x $ is $c\widehat{\beta }_{1}.$

\item When $x=0$, the intercept is $c\widehat{y}$.

\item So $\widehat{\beta }_{0}$ and $\widehat{\beta }_{1}$ are multiplied by 
$c$ when the DV is multiplied by $c$.

\item $R^{2}$ does not change.
\end{itemize}
\end{itemize}

\subsection{\protect\bigskip }

\section{Lecture 13}

\subsection{Moving from Description to Inference}

\begin{itemize}
\item As I've said (over and over), all we've done so far is talked about
the regression line $\widehat{y}=\widehat{\beta }_{0}+\widehat{\beta }_{1}x$
as a description of how well the relationship between $x$ and $y$ can be
approximated by a linear relationship. \ In this context,\ $\widehat{\beta }%
_{0}$ and $\widehat{\beta }_{1}$ are simply descriptive statistics, like the
empirical mean or empirical variance of the observed values of a variable.

\item Now we move to using the formula for the least squares line to make 
\textbf{inferences }about an underlying population from the sample under
analysis. \ Just as we used the statistic $\overline{Y}$ to make inferences
about the parameter $\mu ,$ we will now use the statistics $\widehat{\beta }%
_{1}=\frac{cov\left( x,y\right) }{var(x)}$ and $\widehat{\beta }_{0}=%
\overline{y}-\widehat{\beta }_{1}\overline{x}$ to make inferences about the
parameters $\beta _{0}$ and $\beta _{1}.$

\item As usual, we would like to find unbiased, relatively low-variance
estimators of these parameters$.$ \ As it turns out, the formulae for $%
\widehat{\beta }_{0}$ and $\widehat{\beta }_{1}$ that we developed earlier
generate unbiased estimates of the parameters $\beta _{0}$ and $\beta _{1}$
as long as certain assumptions hold. \ We will now develop those
assumptions. \ Later, we will introduce the additional assumptions necessary
to show that $\widehat{\beta }_{0}$ and $\widehat{\beta }_{1}$ are not only
unbiased, but that have smaller variances than any other possible linear
unbiased estimator of $\beta _{0}$ and $\beta _{1}.$

\item \textbf{Assumption 1: \ The relationship between }$x$\textbf{\ and }$y$%
\textbf{\ in the population is linear, and it is probabilistic. \ }

\begin{itemize}
\item We begin with the assumption of a linear relationship between $x$ and $%
y$. \ This not only rules out curvilinear relationships (draw). \ But
(perhaps more importantly), it rules out relationships in which there are
diminishing returns to x (draw) or in which the effects of x are smaller at
its extreme values (draw sigmoidal function). \ (Ask for examples of these
relationships.)

\item In many cases, these alternate functional forms describe the
relationship between x and y with much more verisimilitude than a linear
functional form. \ As we will see, there are all kinds of ways to account
for these nonlinearities; but for now we are stuck in linearities: we assume
that the change in y associated with a one-unit change in x is the same
across the entire range of $x$.

\item Furthermore, we assume that the linear relationship between $x$ and $y$
is not deterministic -- that is, it is not always the case that a particular 
$y_{i}=\beta _{0}+\beta _{1}x_{i}.$ \ Rather, we assume that the linear
relationship is \textbf{probabilistic}, and therefore write the complete
population model as%
\begin{equation*}
y=\beta _{0}+\beta _{1}x+u,\text{ }
\end{equation*}
\end{itemize}

and we will say that the relationship between any $x_{i}$ and $y_{i}$ in the
population may be written%
\begin{equation*}
y_{i}=\beta _{0}+\beta _{1}x_{i}+u_{i}.\text{ }
\end{equation*}%
Here, $u$ stands for \textquotedblleft unexplained,\textquotedblright\ and\
in this class, we will call $u$ the \textbf{error}.\ \ For a unit $i$ in the
population, $u_{i}$ is the amount by which the sum $\beta _{0}+\beta
_{1}x_{i}$ comes in less than $y_{i}.$ \ It is thus the variation in $y$
which is unexplained by $x$.

\item We view $y$, $x$ and $u$ as random variables when stating this model.

\item Note the difference between $u_{i}$ and $\widehat{u}_{i}:$

\begin{itemize}
\item $u_{i}$ is the \textit{error} associated with a hypothetical unit in
the population. \ It is never observed.

\item $\widehat{u}_{i}$ is the \textit{residual }associated with an observed
unit in our sample once we've estimated a regression line. \ It is the
"mistake" our estimated regression makes as it predicts $y$.
\end{itemize}

\item Note that I have removed the \textquotedblleft
hats,\textquotedblright\ because we are now talking about $\beta _{0}$ and $%
\beta _{1}$ as parameters of interest. \ $\beta _{0}$ and $\beta _{1}$ are
unknown and never directly observed. \ Pause: what does it mean to talk
about these values as parameters? \ i.e., what is $\beta _{1}?$(it's the
number of units of change in $y$ associated, on average, with a one-unit
change in $x$ in the population.) \ what is $\beta _{0}?$ (it is the average
value of $y$ when $x$ equals zero in the population.)

\item \textbf{Assumption 2: \ we have a random sample of }$(x,y)$ \textbf{%
pairs from the population, making them i.i.d.}

\begin{itemize}
\item Although this assumption can often fail to hold in practice, let's
note it for now and move on. \ Note that this is no stronger of an
assumption than we needed in order to say that univariate estimators (such
as Y-bar) are unbiased.
\end{itemize}

\item \textbf{Assumption 3: \ The variance of our observed }$\mathbf{x}%
^{\prime }s$\textbf{\ is nonzero.}

\begin{itemize}
\item Happily, this is a weak assumption and can be easily confirmed by
examining our sample. \ 
\end{itemize}

\item \textbf{Assumption 4: \ The error }$u$\textbf{\ has the expected value
of zero, no matter what the value of }$\mathbf{x}$\textbf{. \ That is, }$%
E(u|x)=0$, or (more intuitively) $cov\left( x,u\right) =0$\textbf{. \ }

\begin{itemize}
\item This is known as the assumption of \textquotedblleft zero conditional
mean.\textquotedblright\ \ It is a statement about the unexplained factors
contained in $u_{i},$ and it asserts that these other factors are unrelated
to $x_{i}$ in that, given a value of $x_{i}$, the mean of the distribution
of these other factors equals zero.

\item Note that this is an assumption about the population: that is, an
assumption about the process generating $y$.

\item It is thus a very strong assumption. \ It means that these other
factors are uncorrelated with $x$. It practical terms, it means that there
are no confounds that could render the relationship between $x$ and $y$
spurious.

\begin{itemize}
\item Example from $y=$ income and $x=$ education.

\item When we write the population model%
\begin{equation*}
\text{income = }\beta _{0}+\beta _{1}\text{education+}u,
\end{equation*}

\item we are assuming that 
\begin{eqnarray*}
E(u|\text{education}) &=&0,\text{ or more to he point} \\
cov\left( \text{education, }u\right) &=&0.
\end{eqnarray*}

\item But can we think about a factor that winds up in $u$ (that is, the
factor helps to explain income) but is correlated with education? \ Three
prominent examples include: parents' education, ability, motivation. To the
extent that these factors explain $y$ and are correlated with $x$,
Assumption 4 does not hold. \ [Perhaps draw causal path diagram on board to
illustrate.]

\item We'll talk about this in detail when we move to multivariate
regression. \ \ 

\item BTW, can we test this assumption by seeing if $corr(x_{i},\widehat{u}%
_{i})=0?$ \ 

\begin{itemize}
\item No: the assumption is about errors in the population, not the
residuals in our sample. \ (What conclusion can we draw if we observe $%
corr(x_{i},\widehat{u}_{i})=0?$ \ (Nothing! \ This is by construction in
OLS!)
\end{itemize}
\end{itemize}

\item A technical aside: the assumption $E(u|x)=0$ indicates that in the
statistical derivations we are about to do, we treat $x$ as fixed. \
Technically, this isn't true as typically we are working with a random
sample of $\left( x,y\right) $ pairs. \ But nothing is lost in the
derivations by treating the $x$ as nonrandom. \ 
\end{itemize}
\end{itemize}

\subsection{The Unbiasedness of the OLS estimator for $\protect\beta _{1}$}

\begin{itemize}
\item These four assumptions together allow us to say that $\widehat{\beta }%
_{0}$ and $\widehat{\beta }_{1}$ that we developed earlier generate unbiased
estimates of the parameters $\beta _{0}$ and $\beta _{1}.$ \ In this
context, we call $\widehat{\beta }_{0}$ and $\widehat{\beta }_{1}$ the 
\textbf{ordinary least squares (OLS) }estimators of $\beta _{0}$ and $\beta
_{1}.$

\item Recall the formula for $\widehat{\beta }_{1}:$ 
\begin{equation*}
\widehat{\beta }_{1}=\frac{S_{xy}}{S_{xx}}=\frac{\sum \left( x_{i}-\overline{%
x}\right) \left( y_{i}-\overline{y}\right) }{\sum \left( x_{i}-\overline{x}%
\right) ^{2}}
\end{equation*}

\item In order for $\widehat{\beta }_{1}$ to be defined, we need Assumption
3: $var\left( x\right) >0.$

\item Note that we can rewrite%
\begin{eqnarray*}
\sum \left( x_{i}-\overline{x}\right) \left( y_{i}-\overline{y}\right)
&=&\sum \left( x_{i}-\overline{x}\right) y_{i}-\sum \left( x_{i}-\overline{x}%
\right) \overline{y} \\
&=&\sum \left( x_{i}-\overline{x}\right) y_{i}-\left[ \sum x_{i}\overline{y}%
-\sum \overline{x}\overline{y}\right] \\
&=&\sum \left( x_{i}-\overline{x}\right) y_{i}-\left( n\overline{x}\overline{%
y}-n\overline{x}\overline{y}\right) \\
&=&\sum \left( x_{i}-\overline{x}\right) y_{i}
\end{eqnarray*}

\item So%
\begin{eqnarray*}
\widehat{\beta }_{1} &=&\frac{\sum \left( x_{i}-\overline{x}\right) y_{i}}{%
\sum \left( x_{i}-\overline{x}\right) ^{2}} \\
&=&\frac{\sum \left( x_{i}-\overline{x}\right) \left( \beta _{0}+\beta
_{1}x_{i}+u_{i}\right) }{SST_{x}} \\
&=&\frac{\beta _{0}\sum \left( x_{i}-\overline{x}\right) +\beta _{1}\sum
\left( x_{i}-\overline{x}\right) x_{i}+\sum \left( x_{i}-\overline{x}\right)
u_{i}}{SST_{x}}
\end{eqnarray*}

\item Note that $\sum \left( x_{i}-\overline{x}\right) =0,$ and 
\begin{eqnarray*}
\sum \left( x_{i}-\overline{x}\right) x_{i} &=&\sum \left( x_{i}^{2}-%
\overline{x}x_{i}\right) \\
&=&\sum x_{i}^{2}-\overline{x}\sum x_{i} \\
&=&\sum x_{i}^{2}-n\left( \overline{x}\right) ^{2} \\
&=&\sum x_{i}^{2}-2n\left( \overline{x}\right) ^{2}+n\left( \overline{x}%
\right) ^{2} \\
&=&\sum x_{i}^{2}-2\overline{x}\sum x_{i}+\sum \left( \overline{x}\right)
^{2}\text{ [since }n\overline{x}=\sum x_{i},\text{ and }n\left( \overline{x}%
\right) ^{2}=\sum \left( \overline{x}\right) ^{2}\text{]} \\
&=&\sum x_{i}^{2}-2\overline{x}x_{i}+\left( \overline{x}\right) ^{2} \\
&=&\sum \left( x_{i}-\overline{x}\right) ^{2}=SST_{x}.
\end{eqnarray*}

\item So:%
\begin{eqnarray*}
\widehat{\beta }_{1} &=&\frac{\beta _{1}SST_{x}+\sum \left( x_{i}-\overline{x%
}\right) u_{i}}{SST_{x}} \\
&=&\beta _{1}+\frac{\sum \left( x_{i}-\overline{x}\right) u_{i}}{SST_{x}}
\end{eqnarray*}

\item Now let's find the expected value of $\widehat{\beta }_{1}:$%
\begin{eqnarray*}
E\left( \widehat{\beta }_{1}\right) &=&E\left[ \beta _{1}+\frac{\sum \left(
x_{i}-\overline{x}\right) u_{i}}{SST_{x}}\right] \\
\text{Employing Assumptions 2 and 4, we can proceed} &\text{:}& \\
&=&\beta _{1}+\frac{1}{SST_{x}}E\left[ \sum \left( x_{i}-\overline{x}\right)
u_{i}\right] \\
&=&\beta _{1}+\frac{1}{SST_{x}}\sum \left( x_{i}-\overline{x}\right) E\left(
u_{i}\right) \\
&=&\beta _{1}+\frac{1}{SST_{x}}\sum \left( x_{i}-\overline{x}\right) 0 \\
&=&\beta _{1}.
\end{eqnarray*}

\item And for $\widehat{\beta }_{0}:$%
\begin{equation*}
\widehat{\beta }_{0}=\overline{y}-\widehat{\beta }_{1}\overline{x}
\end{equation*}%
\begin{eqnarray*}
\text{ Since }y_{i} &=&\beta _{0}+\beta _{1}x_{i}+u_{i},\text{ then }%
\overline{y}=\beta _{0}+\beta _{1}\overline{x}+\overline{u}\text{,
substituting:} \\
\widehat{\beta }_{0} &=&\beta _{0}+\beta _{1}\overline{x}+\overline{u}-%
\widehat{\beta }_{1}\overline{x} \\
\widehat{\beta }_{0} &=&\beta _{0}+\left( \beta _{1}-\widehat{\beta }%
_{1}\right) \overline{x}+\overline{u} \\
E\left( \widehat{\beta }_{0}\right) &=&E\left[ \beta _{0}+\left( \beta _{1}-%
\widehat{\beta }_{1}\right) \overline{x}+\overline{u}\right] \text{ \ Again
using Assumptions 2 and 4, we proceed:} \\
&=&\beta _{0}+\left[ \beta _{1}-E\left( \widehat{\beta }_{1}\right) \right] 
\overline{x}+E\left( \overline{u}\right) \text{ \ } \\
&=&\beta _{0}+\left[ \beta _{1}-\beta _{1}\right] \overline{x}+E\left( 
\overline{u}\right) \text{ \ \ } \\
&=&\beta _{0}\text{ \ \ }
\end{eqnarray*}
\end{itemize}

\subsection{The variances of the OLS estimators}

\begin{itemize}
\item Last time, we showed how four assumptions regarding x and y...

\begin{enumerate}
\item Relationship is linear and probabilistic

\item x,y drawn from random sample, making them i.i.d.

\item variance of x is nonzero

\item $E(u|x)=0$
\end{enumerate}

\item ..resulted in the least-squares solutions for $\widehat{\beta }_{0}$
and $\widehat{\beta }_{1}$ being unbiased estimators for $\beta _{0}$ and $%
\beta _{1}.$

\item $\widehat{\beta }_{0}$ and $\widehat{\beta }_{1}$ are statistics, just
like (say) $\overline{Y}$. \ Now, just as we did when making inferences
about univariate distributions, we can say something about not only the
unbiasedness of our estimates but also how far we can expect them to be away
from the true parameter on average. \ In the univariate context, we were
interested in describing the sampling distribution of the statistic X-bar.

\item We will do the same thing here and consider the sampling distribution
of $\widehat{\beta }_{0}$ and $\widehat{\beta }_{1}.$ \ We, of course,
already know the means of these sampling distributions: they are $\beta _{0}$
and $\beta _{1}.$ \ Let's now compute the variances of $\widehat{\beta }_{0}$
and $\widehat{\beta }_{1}.$ \ To compute these , we make a fifth assumption\
[add to list under separate heading]:

\item \textbf{Assumption 5: }$VAR(u|x)=\sigma ^{2}.$ \ That is, the error $u$
has the same variance no matter what the value of $x$. \ This is also known
as homoskedasticity. \ When the assumption is violated, we have
\textquotedblleft heteroskedasticity.\textquotedblright

\item Draw pictures on board: Wooldridge p. 54 and p.55 -- to illustrate
homoskedasticity and heteroskedasticity.

\begin{itemize}
\item Note difference between $VAR(u|x)=\sigma ^{2}$ and $E(u|x)=0.$

\item We did not need Assumption 5 to establish the unbiasedness of the OLS
estimators.
\end{itemize}

\item Note that this now allows us to write:%
\begin{eqnarray*}
y &=&\beta _{0}+\beta _{1}x+u\text{ \ [the model]} \\
E(y|x) &=&E\left[ \left( \beta _{0}+\beta _{1}x+u\right) |x\right] \text{ \
[taking expectations conditional on x]} \\
&=&E\left( \beta _{0}|x)+E(\beta _{1}x|x)+E(u|x\right) \\
E(y|x) &=&\beta _{0}+\beta _{1}x+0\text{ \ [thanks to assumption 4]}
\end{eqnarray*}

\item and%
\begin{eqnarray*}
VAR(y|x) &=&VAR\left[ \left( \beta _{0}+\beta _{1}x+u\right) |x\right] \\
&=&0+0+VAR(u|x) \\
&=&\sigma ^{2}.\text{ \ [by assumption 5]}
\end{eqnarray*}

\item What is $\sigma ^{2}?$

\begin{itemize}
\item Ask class.

\item Note that it is not $VAR(y).$ \ It is $VAR(y|x).$

\item Note that it is a parameter-not an estimate. \ So it's something in
the population, not the sample.

\item $\sigma ^{2}$ is a measure of the extent to which unexplained factors
are affecting the value of y. \ 

\begin{itemize}
\item Assumption 4 means that we assume that these factors are unrelated to
x.

\item Assumption 5 means that these factors are constant regardless of the
value of x.

\item When $\sigma ^{2}$ is bigger, it is the case that other factors
explain a great deal of the variation in y in addition to x.

\item When $\sigma ^{2}$ is smaller, it is the case that x is explaining a
great deal of the variation in y on its own.
\end{itemize}
\end{itemize}

\item Later, we'll show how to estimate the variances of the OLS estimators
when Assumption 5 is violated.

\item Now, we're ready to derive the sampling variances of the OLS
estimators. \ It is the case that:%
\begin{eqnarray*}
VAR\left( \widehat{\beta }_{1}\right) &=&\frac{\sigma ^{2}}{SST_{x}}; \\
VAR\left( \widehat{\beta }_{0}\right) &=&\frac{\sigma ^{2}\frac{%
\sum\nolimits_{i}x_{i}^{2}}{n}}{SST_{x}}.
\end{eqnarray*}

\item Proof:

\begin{itemize}
\item Recall that in the proof establishing the unbiasedness of $\widehat{%
\beta }_{1},$ we wrote 
\begin{equation*}
\widehat{\beta }_{1}=\beta _{1}+\frac{\sum \left( x_{i}-\overline{x}\right)
u_{i}}{SST_{x}}.
\end{equation*}

\item We then used the conditional mean assumption in order to say that the
value of the last term of this expression is zero.

\item Now let's consider%
\begin{eqnarray*}
VAR\left( \widehat{\beta }_{1}\right) &=&VAR\left[ \beta _{1}+\frac{\sum
\left( x_{i}-\overline{x}\right) u_{i}}{SST_{x}}\right] . \\
&=&0+\frac{1}{\left( SST_{x}\right) ^{2}}VAR\left[ \sum \left( x_{i}-%
\overline{x}\right) u_{i}\right] \text{ \ [fixed X means we can treat as
constant]} \\
&=&\frac{1}{\left( SST_{x}\right) ^{2}}\sum \left( x_{i}-\overline{x}\right)
^{2}VAR\left( u_{i}\right) \text{ \ [again]} \\
&=&\frac{1}{\left( SST_{x}\right) ^{2}}\sum \left( x_{i}-\overline{x}\right)
^{2}\sigma ^{2}\text{ \ [by Assumption 5]} \\
&=&\frac{SST_{x}}{\left( SST_{x}\right) ^{2}}\sigma ^{2}=\frac{\sigma ^{2}}{%
SST_{x}}.
\end{eqnarray*}
\end{itemize}

\item I'll leave the proof of the variance of $\widehat{\beta }_{0}$ as an
exercise.

\item Let's have a look at $VAR\left( \widehat{\beta }_{1}\right) .$ We'd
obviously like this to be as (what?) small as possible. \ What are the two
quantities that make it small?

\begin{itemize}
\item $\sigma ^{2}:$ as gets smaller, $VAR\left( \widehat{\beta }_{1}\right) 
$ gets smaller.

\item $SST_{x}:$ as gets bigger, $VAR\left( \widehat{\beta }_{1}\right) $
gets smaller.

\begin{itemize}
\item Now let's take a closer look at $SST_{x}:$%
\begin{eqnarray*}
SST_{x} &=&\sum \left( x_{i}-\overline{x}\right) ^{2} \\
\frac{SST_{x}}{n} &=&\frac{\sum \left( x_{i}-\overline{x}\right) ^{2}}{n} \\
\frac{SST_{x}}{n} &=&var(x) \\
SST_{x} &=&n\cdot var(x)
\end{eqnarray*}

\item So%
\begin{equation*}
VAR\left( \widehat{\beta }_{1}\right) =\frac{\sigma ^{2}}{SST_{x}}=\frac{%
\sigma ^{2}}{n\cdot var(x)}.
\end{equation*}
\end{itemize}
\end{itemize}

\item Of these three properties, what can we really do anything about?

\item In most cases, only $n$:

\begin{itemize}
\item $\sigma ^{2}$ is a parameter; it's declines only to the extent that x
explains y\ well.

\item $var(x)$ is the empirical variance of $x$ in our sample. \ In a random
sample, it will of course look like the variance of $x$ in the population. \
Not a whole lot we can do about that, either.

\item So what does this say about the relationship between sample size and
the variance of $\widehat{\beta }_{1}?$
\end{itemize}

\item Remember $VAR(\overline{Y})=\frac{\sigma ^{2}}{n}?$ \ Compare to $%
\frac{\sigma ^{2}}{n\cdot var(x)}.$ \ Again, we have a ratio of something we
can't control (the variance of y) over something we can (n).
\end{itemize}

\section{Lecture 15}

\subsection{Estimating the error variance}

\begin{itemize}
\item You'll recall that in the univariate context, we encountered a
roadblock when we wrote $VAR(\overline{Y})=\frac{\sigma _{Y}^{2}}{n}.$ \
That is that we rarely know $\sigma _{Y}^{2}.$ \ Well, when we write $%
VAR\left( \widehat{\beta }_{1}\right) =\frac{\sigma ^{2}}{SST_{x}},$ we have
the same problem. \ We rarely have reason to know $\sigma ^{2}$ in the OLS
context, either. \ 

\item What did we do in the univariate case? \ We estimated $\sigma _{Y}^{2}$
with $S_{U}^{2}=\frac{\sum\nolimits_{i}\left( y_{i}-\overline{y}\right) ^{2}%
}{n-1}.$ \ You'll recall that this was the empirical variance of $y$
adjusted for the number of degrees of freedom (one) used in generating the
estimate.

\item Well, we'll do a similar thing here. \ We will estimate $\sigma ^{2}$
with 
\begin{equation*}
\widehat{\sigma }^{2}=\frac{\sum \widehat{u}_{i}^{2}}{(n-2)}=\frac{SSR}{%
\left( n-2\right) }.
\end{equation*}

\item A proof that $E(\widehat{\sigma }^{2})=\sigma ^{2}$ may be found on
p.57 of Wooldridge. \ The intuition here is that we have the variance of the
residuals, again adjusted by the number of degrees of freedom (two) -- since
we've already generated estimates $\left( \widehat{\beta }_{0}\text{ and }%
\widehat{\beta }_{1}\right) $ of two parameters using the two first order
conditions for deriving the OLS estimators, which required that:%
\begin{equation*}
\sum \widehat{u}_{i}=0\text{ \ and \ }\sum \widehat{u}_{i}x_{i}=0.
\end{equation*}

\item The way to think about this (or any degrees of freedom scenario) is:
how many pieces of data are free to vary once we've made our estimate? \
Here, if we know $n-2$ of the residuals, we can always calculate the other
two residuals via the formulas above. \ They are not free to vary. \ We
therefore lose two degrees of freedom, resulting in a total of $n-2$ degrees
of freedom in our estimate of $\sigma ^{2}.$

\item Thus our unbiased estimators of $VAR\left( \widehat{\beta }_{1}\right) 
$ and $VAR\left( \widehat{\beta }_{0}\right) $ are:%
\begin{eqnarray*}
\widehat{VAR\left( \widehat{\beta }_{1}\right) } &=&\frac{\widehat{\sigma }%
^{2}}{SST_{x}}=\frac{\frac{SSR}{\left( n-2\right) }}{SST_{x}} \\
\widehat{VAR\left( \widehat{\beta }_{0}\right) } &=&\frac{\widehat{\sigma }%
^{2}\frac{\sum\nolimits_{i}x_{i}^{2}}{n}}{SST_{x}}=\frac{\frac{SSR}{\left(
n-2\right) }\frac{\sum\nolimits_{i}x_{i}^{2}}{n}}{SST_{x}}.
\end{eqnarray*}

\item $\widehat{\sigma }^{2},$ our estimate of $\sigma ^{2},$ plays another
important role, because 
\begin{equation*}
\sqrt{\widehat{\sigma }^{2}}=\widehat{\sigma }\overset{p}{\rightarrow }%
\sigma .
\end{equation*}

\item Thus $\widehat{\sigma }$ is an interesting quantity in and of itself.
\ It is expressed in units of $y$, which means that it tells us:

\begin{itemize}
\item empirically, how far off the typical fitted value of y is away from
the observed value; and

\item theoretically, the extent to which unexplained factors are affecting
the value of y. \ 
\end{itemize}

\item It is a very informative statistic that gets much less attention than
it deserves. \ 

\item Terminology:

\begin{itemize}
\item Wooldridge calls $\widehat{\sigma }$ the Standard Error of the
Regression (SER).

\item In Stata's regression output, $\widehat{\sigma }$ is displayed as
"Root MSE," which stands for the root of the mean squared error of the
regression. \ 

\item I call $\widehat{\sigma }$ the standard error of the estimate, or SEE.

\item And sometimes you'll just see it displayed as $\widehat{\sigma }.$
\end{itemize}

\item \lbrack NEXT YEAR: RELATIONSHIP BETWEEN $R^{2}$ AND $\widehat{\sigma }%
.]$
\end{itemize}

\subsection{Hypothesis tests about $\protect\beta _{1}$}

\begin{itemize}
\item For now, we'll hold off on a discussion of how to conduct hypothesis
tests on $\beta _{1}.$ \ It will be more efficient to turn to it once we
encounter multiple regression in the next lecture. \ 
\end{itemize}

\subsection{Controlling for a variable}

\begin{itemize}
\item We are about to move on to multivariate regression.

\item But before we do that, let's motivate the notion of controlling for a
variable, and noticing how this does and does not compare to multiple
regression.

\item As we conduct research on political phenomena, we are often interested
in what is known as the \textit{ceteris paribus}--that is, the "all things
being equal"--relationship between $X$ and $Y$. \ [Draw diagram on board.]

\begin{itemize}
\item That is, we are interested in the (often counterfactual case) of what
the relationship between X and Y would look like if all other aspects of our
units were the same.

\begin{itemize}
\item We often call those other aspects variables $Z$.
\end{itemize}
\end{itemize}

\item \lbrack NEXT YEAR: WHY IS THIS A PROBLEM? \ BECAUSE IF $Z$ IS
CORRELATED WITH BOTH X AND Y, THEN THE BIVARIATE RELATIONSHIP BETWEEN X AND
Y MAY LEAD US TO IMPROPER CONCLUSIONS ABOUT THE CETERIS PARIBUS RELATIONSHIP
BETWEEN X AND Y.

\begin{itemize}
\item MAYBE INCLUDE EXAMPLES WITH CORRELATIONS?

\item Sometimes we do this because we are interested in the effect of X on
Y, and we want to be sure that it is not due to $Z$.

\item But often, we're simply interested in the relationship between X and
Y, holding everything else constant.
\end{itemize}

\item Let's get specific about the terminology used here:

\begin{itemize}
\item In this context, $Z$ is called the potential \textbf{confound.}

\item If $Z$ confounds the relationship between $X$ and $Y$, it \textit{%
renders the relationship spurious}.

\begin{itemize}
\item That is, it leads us to improper conclusions about the \textit{ceteris
paribus}--that is, the "all things being equal"--relationship between $X$
and $Y$.
\end{itemize}

\item Let's think a bit about potential confounds that may render a
relationship spurious:\FRAME{ftbpF}{3.2655in}{2.4561in}{0pt}{}{}{Figure}{%
\special{language "Scientific Word";type "GRAPHIC";maintain-aspect-ratio
TRUE;display "USEDEF";valid_file "T";width 3.2655in;height 2.4561in;depth
0pt;original-width 5.0004in;original-height 3.7498in;cropleft "0";croptop
"1";cropright "1";cropbottom "0";tempfilename
'MVPFV505.wmf';tempfile-properties "XPR";}}

\item To determine whether Z renders the relationship between X and\ Y
spurious, we:

\begin{itemize}
\item "control for $Z$"

\item "condition on $Z$"

\item "hold $Z$ constant."
\end{itemize}

\item All three of these phrases typically mean the same thing.

\item But there are several different ways to do this. \ Ideally, we would
do exactly what "holding Z constant" suggests: divide our units by
categories of Z and examine the relationship between X and Y within each
category of Z.

\begin{itemize}
\item If the relationship persists after controlling for Z, we say that it
is not spurious.

\item If it no longer persists, we say that Z is a confound rendering the
relationship between X and\ Y spurious.
\end{itemize}

\item In practice, we usually do something much less careful.

\item Handout: controlling for a variable.
\end{itemize}
\end{itemize}

\section{Lecture 16}

Although controlling for a variable by adding Z as an additive term in a
multiple regression seems overly simple, it can still provide us with
unbiased estimates of the ceteris paribus relationship between X and Y.

\begin{itemize}
\item 
\begin{itemize}
\item To see this, let's first analyze what happens when we don't control
for Z:
\end{itemize}

\item Assume that the true model is%
\begin{equation*}
y=\beta _{0}+\beta _{1}x+\beta _{2}z+\nu
\end{equation*}

where $u$ is an error term such that $cor\left( u|x,z\right) =0$. \
Regressing y on x1 and x2 will yield unbiased, consistent estimates of $%
\beta .$

\item Notice that we're making a big assumption here: no interaction between
x and z, and z enters into the DGP in a linear fashion.

\item But if instead we regress y only on x1, obtaining the equation 
\begin{equation*}
y=\beta _{0}+\beta _{1}x+u,
\end{equation*}

then what we are really doing is moving $\beta _{2}x_{2}$ to the error term, 
$\nu :$%
\begin{eqnarray*}
y &=&\beta _{0}+\beta _{1}x+(\beta _{2}z+\nu )\mathbf{,} \\
\text{ where }u &=&(\beta _{2}z+\nu ).
\end{eqnarray*}

\item You'll recall that in the bivariate case, 
\begin{equation*}
\widehat{\beta }_{1}=\beta _{1}+\frac{\sum \left( x_{i}-\overline{x}\right)
u_{i}}{SST_{x}},
\end{equation*}

\item When then rely on the assumption that the covariance of x and u is
zero to make the final term dissappear, and thus say that $E\left( \widehat{%
\beta }_{1}\right) =\beta _{1}.$ \ But now consider%
\begin{equation*}
\widehat{\beta }_{1}=\beta _{1}+\frac{\sum \left( x_{i}-\overline{x}\right)
\left( \beta _{2}z_{i}+\nu \right) }{SST_{x}}.
\end{equation*}

\item Taking expectations, we now have%
\begin{eqnarray*}
E\left( \widehat{\beta }_{1}\right) &=&E\left( \beta _{1}\right) +E\left[ 
\frac{\sum \left( x_{i}-\overline{x}\right) \left( \beta _{2}z_{i}+\nu
\right) }{SST_{x_{1}}}\right] \\
&=&\beta _{1}+\frac{\sum \left( x_{1i}-\overline{x}\right) E\left[ \left(
\beta _{2}z_{i}+\nu \right) \right] }{SST_{x}} \\
&=&\beta _{1}+\beta _{2}\left[ z_{i}\frac{\sum \left( x_{i}-\overline{x}%
\right) }{SST_{x}}\right] .
\end{eqnarray*}

\item It turns out that $z_{i}\frac{\sum \left( x_{i}-\overline{x}\right) }{%
SST_{x}}=\frac{cov(x,z)}{var(x)},$ which is the slope coefficient we would
obtain if we regressed z on x! \ 

\item What if we wanted to say something about the sign of the bias? \ Well,
note that $sign\left[ \frac{cov(x,z)}{var(x)}\right] =sign\left[ cov(x,z)%
\right] .$ \ So if we omit x2 from our equation, we can now say that its
sign is 
\begin{equation*}
sign\left[ cov(x,z)\times \beta _{2}\right]
\end{equation*}%
\ 

\item What does this mean in practice? \ Consider a regression in which you
model feelings toward Barack Obama as a function of Democratic Party
identification. \ You omit a dummy variable for whether an individual is
African-American. \ In what direction is your estimate of $\beta _{1}$
almost assuredly biased?

\item What happens if $cov(x,z)=0?$ \ What happens if $\beta _{2}=0?$ \ 

\begin{itemize}
\item That's right: when a variable is omitted, TWO\ problems must be
present in order for it to cause bias:

\begin{enumerate}
\item it is correlated with one or more x's in your model.

\item its partial effect on y is not zero.
\end{enumerate}

\item Why, then, do we love randomly assigning individuals to $x$? \
Because\ by construction, $cov(x,z)$ (for any omitted $z$ you can think of)
is zero, making $\widehat{\beta }_{1}$unbiased.
\end{itemize}

\item This is a nice simple example, but it gets more complicated in a
multivariate context. \ You'll see that next time.

\begin{itemize}
\item \lbrack That's because the term $\beta _{2}\left[ x_{2}\frac{\sum
\left( x_{1i}-\overline{x_{1}}\right) }{SST_{x_{1}}}\right] $ becomes $\beta
_{2}\left[ \left( \frac{1}{N}X^{\prime }X\right) ^{-1}\left( \frac{1}{N}%
X^{\prime }x_{2}\right) \right] ,$ which takes into account the extent to
which the omitted variable $\left( x_{2}\right) $ is collinear with all the
included x's in the model. \ In practice, the sign of this bias is hard to
consider in such a back-of-the-envelope fashion.]
\end{itemize}

\item Take-home-point: if you leave out a variable that is BOTH correlated
with included $x$'s and has a separate effect on y, your estimates will
suffer from omitted variable bias.
\end{itemize}

\end{document}
