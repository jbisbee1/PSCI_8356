
\documentclass[11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[abbr]{harvard}
\usepackage{amssymb}
\usepackage{setspace,graphics,epsfig,amsmath,rotating,amsfonts,mathpazo}

\setcounter{MaxMatrixCols}{10}

\topmargin=0 in \headheight=0in \headsep=0in \topskip=0in \textheight=9in \oddsidemargin=0in \evensidemargin=0in \textwidth=6.5in
\setcounter{section}{0}

\begin{document}


\singlespacing

\noindent \textbf{NYU Department of Politics - Quant I}

\noindent \textbf{Prof. Patrick Egan}

\bigskip

\begin{center}
\Large{\textbf{Some Helpful Results Regarding the Math of\ Expectations }}

\end{center}

\section{The basics: functions of the random variable $Y$}

\bigskip In all cases, $Y$ is assumed to be a discrete random variable with
a probability function $p(y)$ that accurately characterizes a population
frequency distribution. \ With the exception of the final result, proofs are
omitted. \bigskip 

\subsection{Definitions}

\bigskip

\begin{eqnarray*}
E(Y) &\equiv &\sum_{y}yp(y)=\mu . \\
&& \\
VAR(Y) &\equiv &E[(Y-\mu )^{2}]=\sigma ^{2}.
\end{eqnarray*}%
\bigskip

\subsection{Functions of a random variable\protect\bigskip}

\begin{itemize}
\item Where $g(Y)$ is a real-valued function of $Y$,
\end{itemize}

\begin{equation*}
E[g(Y)]=\sum_{y}g(y)p(y).
\end{equation*}

\begin{itemize}
\item Where $g_{1},g_{2}...g_{k}$ are $k$ real-valued functions of $Y$, we
can \textquotedblleft distribute expectations\textquotedblright :
\end{itemize}

\begin{equation*}
E[g_{1}(Y)+g_{2}(Y)+...+g_{k}(Y)]=E[g_{1}(Y)]+E[g_{2}(Y)]+...+E[g_{k}(Y)].
\end{equation*}%
\bigskip

\subsection{Constants\protect\bigskip}

\begin{itemize}
\item Where $c$ is a constant,%
\begin{equation*}
E(c)=c,
\end{equation*}

and%
\begin{equation*}
E[cg(Y)]=cE[g(Y)].
\end{equation*}

\item Population parameters, such as $\mu $ and $\sigma ^{2},$ are
constants.\bigskip 
\end{itemize}

\subsection{The variance of a random variable}

\medskip \bigskip

\begin{equation*}
\sigma ^{2}=E(Y^{2})-\mu ^{2}.
\end{equation*}

Proof:

\begin{eqnarray*}
\sigma ^{2} &=&VAR(Y)\equiv E[(Y-\mu )^{2}] \\
&=&E(Y^{2}+\mu ^{2}-2Y\mu )\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ [Expanding the quadratic]} \\
&=&E(Y^{2})+E(\mu ^{2})-E(2Y\mu )\text{ \ \ \ \ \ \ \ \ \ \ \ [Distributing
expectations]} \\
&=&E(Y^{2})+\mu ^{2}-2\mu E(Y)\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ [}2%
\text{, and }\mu \text{ (a population parameter) are constants]} \\
&=&E(Y^{2})+\mu ^{2}-2\mu \mu \text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ \ [}E(Y)=\mu \text{]} \\
&=&E(Y^{2})-\mu ^{2}\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ \ \ \ \ \ \ \ \ }\square .
\end{eqnarray*}

Note this means we can also write%
\begin{equation*}
\sigma ^{2}=VAR(Y)=E(Y^{2})-\left[ E\left( Y\right) \right] ^{2}.
\end{equation*}

\section{More advanced: Functions of Random Variables $Y_1, Y_2... Y_N$}

\bigskip In all cases, $Y_1, Y_2... Y_N$ are assumed to be random
variables with respective means $\mu _{1}$, $\mu _{2}$...$\mu _{N}$ and variances $\sigma
_{1}^{2}$, $\sigma _{2}^{2}...\sigma _{N}^{2}$. \bigskip

\subsection{Definitions}

\bigskip

\begin{eqnarray*}
COV(Y_{1},Y_{2}) &\equiv &E[(Y_{1}-\mu _{1})(Y_{2}-\mu _{2})]. \\
\text{correlation coefficient} &\equiv &\rho _{Y_{1}Y_{2}}\equiv \frac{%
COV(Y_{1},Y_{2})}{\sigma _{1}\sigma _{2}}
\end{eqnarray*}%
\bigskip

\subsection{Decomposing covariance\protect\bigskip}

\begin{equation*}
COV(Y_{1},Y_{2})=E(Y_{1}Y_{2})-E(Y_{1})E(Y_{2})=E(Y_{1}Y_{2})-\mu _{1}\mu
_{2}.
\end{equation*}

Proof:%
\begin{eqnarray*}
COV(Y_{1},Y_{2}) &\equiv &E[(Y_{1}-\mu _{1})(Y_{2}-\mu _{2})] \\
&=&E[Y_{1}Y_{2}-Y_{1}\mu _{2}-Y_{2}\mu _{1}+\mu _{1}\mu _{2}]\text{ \
(cross-multiplying)} \\
&=&E(Y_{1}Y_{2})-E(Y_{1}\mu _{2})-E(Y_{2}\mu _{1})+E(\mu _{1}\mu _{2})\text{
\ (distributing expectations)} \\
&=&E(Y_{1}Y_{2})-\mu _{2}E(Y_{1})-\mu _{1}E(Y_{2})+\mu _{1}\mu _{2}\text{ \ (%
}\mu _{1},\mu _{2}\text{ are constants)} \\
&=&E(Y_{1}Y_{2})-\mu _{2}\mu _{1}-\mu _{1}\mu _{2}+\mu _{1}\mu _{2}\text{ \
(definition of }E(Y)\text{)} \\
&=&E(Y_{1}Y_{2})-\mu _{1}\mu _{2}.\;\square .
\end{eqnarray*}%
\newpage 

\subsection{Covariance of independent random variables\protect\bigskip}

\begin{itemize}
\item If $Y_{1}$, $Y_{2}$ independent, then
\end{itemize}

\begin{equation*}
COV(Y_{1},Y_{2})=0.
\end{equation*}

Proof: \ 

From above,%
\begin{equation*}
COV(Y_{1},Y_{2})=E(Y_{1}Y_{2})-E(Y_{1})E(Y_{2}).
\end{equation*}

But $Y_{1}$,$Y_{2}$ independent$\Rightarrow $ $%
E(Y_{1}Y_{2})=E(Y_{1})E(Y_{2}),$ so%
\begin{eqnarray*}
Y_{1},Y_{2}\text{ independent} &\Rightarrow
&COV(Y_{1},Y_{2})=E(Y_{1})E(Y_{2})-E(Y_{1})E(Y_{2}) \\
&=&0.
\end{eqnarray*}

\begin{itemize}
\item However, the converse is not true. \ That is, $COV(Y_{1},Y_{2})=0$
does not imply independence of $Y_{1}$, $Y_{2}$.\bigskip
\end{itemize}

\subsection{Expected value and variance of linear functions of random variables%
\protect\bigskip}

\begin{itemize}
\item Consider $U_{1}$, a linear function of the random variables $%
Y_{1},Y_{2},...Y_{n}$ and constants $a_{1},a_{2},...a_{n}$,%
\begin{equation*}
U_{1}=a_{1}Y_{1}+a_{2}Y_{2}+...+a_{n}Y_{n}=\sum_{i=1}^{n}a_{i}Y_{i},
\end{equation*}

and similarly%
\begin{equation*}
U_{2}=\sum_{j=1}^{m}b_{j}X_{j},
\end{equation*}

where $Y_{1},Y_{2},...Y_{n}$ are random variables with $E(Y_{i})=\mu _{i}$
and $X_{1},X_{2},...X_{n}$ are random variables with $E(X_{i})=\xi _{i}$
[\textquotedblleft ksi-sub-i\textquotedblright ]. \ Then (1), (2) and (3)
below follow.\bigskip
\end{itemize}

\subsection{Expected value of a function of RVs\protect\bigskip}

\begin{equation*}
E(U_{1})=\sum_{i=1}^{n}a_{i}\mu _{i}.  \tag{1}
\end{equation*}%
Proof:%
\begin{eqnarray*}
E(U_{1}) &=&E(a_{1}Y_{1})+E(a_{2}Y_{2})+...+E(a_{n}Y_{n})\text{ \
(distributing expections)} \\
&=&a_{1}E(Y_{1})+a_{2}E(Y_{2})+...+a_{n}E(Y_{n})\text{ \ (factoring out
constants)} \\
&=&a_{1}\mu _{1}+a_{2}\mu _{2}+...+a_{n}\mu _{n}\text{ \ (definition of
expected value)} \\
&=&\sum_{i=1}^{n}a_{i}\mu _{i}\text{ \ \ }\square .
\end{eqnarray*}%
\newpage

\subsubsection{Variance of a function of RVs}\protect\bigskip 
\protect\begin{equation*}
VAR(U_{1})=\sum_{i=1}^{n}a_{i}^{2}VAR(Y_{i})+2%
\sum_{i<j}a_{i}a_{j}COV(Y_{i},Y_{j}),\text{ }   \tag{2}
\protect\end{equation*}%


where the final sum is over all pairs $(i,j)$ with $i<j$. \ (What does this
mean in practice? \ That the covariance of each pair of RVs is taken only
once under the summation sign.)\medskip

Proof:%
\begin{eqnarray*}
VAR(U_{1}) &\equiv &E\left\{ [U_{1}-E(U_{1})]^{2}\right\} \text{ \ [since }%
U_{1}\text{ is itself a random variable]} \\
&=&E\left[ \left( \sum_{i=1}^{n}a_{i}Y_{i}-\sum_{i=1}^{n}a_{i}\mu
_{i}\right) ^{2}\right] \text{ \ (from above)} \\
&=&E\left[ \left( \sum_{i=1}^{n}a_{i}\left( Y_{i}-\mu _{i}\right) \right)
^{2}\right] \text{ \ (factoring out constant)}
\end{eqnarray*}

Note that the square of a sum always equals the sum of all the squares$+$
sum of all the (2 $\times $cross products). \ So for example%
\begin{eqnarray*}
\left( b_{1}+b_{2}\right) ^{2} &=&\left( b_{1}\right) ^{2}+\left(
b_{2}\right) ^{2}+2b_{1}b_{2} \\
\left( b_{1}+b_{2}+b_{3}\right) ^{2} &=&\left( b_{1}\right) ^{2}+\left(
b_{2}\right) ^{2}+\left( b_{3}\right)
^{2}+2b_{1}b_{2}+2b_{1}b_{3}+2b_{2}b_{3}=\sum_{i=1}^{3}b_{i}^{2}+2%
\sum_{i<j}^{3}b_{i}b_{j}
\end{eqnarray*}

and generally

\begin{equation*}
\left( \sum_{i=1}^{n}b_{i}\right) ^{2}=\left( b_{1}+b_{2}+...+b_{n}\right)
^{2}=\sum_{i=1}^{n}b_{i}^{2}+2\sum_{i<j}^{n}b_{i}b_{j}.
\end{equation*}

\medskip Now we can write

\begin{eqnarray*}
E\left[ \left( \sum_{i=1}^{n}a_{i}\left( Y_{i}-\mu _{i}\right) \right) ^{2}%
\right] &=&E\left[ \sum_{i=1}^{n}a_{i}^{2}\left( Y_{i}-\mu _{i}\right)
^{2}+\sum_{i<j}^{n}2a_{i}a_{j}\left( Y_{i}-\mu _{i}\right) \left( Y_{j}-\mu
_{j}\right) \right] \\
&=&\sum_{i=1}^{n}a_{i}^{2}E\left[ \left( Y_{i}-\mu _{i}\right) ^{2}\right]
+\sum_{i<j}2a_{i}a_{j}E\left[ \left( Y_{i}-\mu _{i}\right) \left( Y_{j}-\mu
_{j}\right) \right] \text{ \ (distributing expectations)} \\
&=&\sum_{i=1}^{n}a_{i}^{2}VAR(Y_{i})+2\sum_{i<j}a_{i}a_{j}COV(Y_{i},Y_{j}).%
\text{ \ (def. of variance and covariance) }\square .
\end{eqnarray*}%
\newpage

\subsubsection{Covariance of two functions of RVs\protect\bigskip}

\begin{equation*}
COV(U_{1},U_{2})=\sum_{i=1}^{n}\sum_{j=1}^{m}a_{i}b_{j}COV(Y_{i},X_{j}). 
\tag{3}
\end{equation*}

Proof:%
\begin{equation*}
COV(U_{1},U_{2})=E\left[ \left(
\sum_{i=1}^{n}a_{i}Y_{i}-\sum_{i=1}^{n}a_{i}\mu _{i}\right) \left(
\sum_{j=1}^{m}b_{j}X_{j}-\sum_{i=1}^{m}b_{j}\xi _{i}\right) \right] \text{ }
\end{equation*}

\begin{center}
(by def. of covariance, since $U_{1},U_{2}$ are themselves RVs)
\end{center}

\begin{eqnarray*}
&=&E\left[ \left( \sum_{i=1}^{n}a_{i}\left( Y_{i}-\mu _{i}\right) \right)
\left( \sum_{j=1}^{m}b_{j}\left( X_{j}-\xi _{i}\right) \right) \right] \text{
(simplifying)} \\
&=&E\left[ \sum_{i=1}^{n}\sum_{j=1}^{m}a_{i}b_{j}\left( Y_{i}-\mu
_{i}\right) \left( X_{j}-\xi _{i}\right) \right] \text{ \ (cross-multiplying)%
} \\
&=&\sum_{i=1}^{n}\sum_{j=1}^{m}a_{i}b_{j}E\left[ \left( Y_{i}-\mu
_{i}\right) \left( X_{j}-\xi _{i}\right) \right] \text{ \ (distributing
expectations)} \\
&=&\sum_{i=1}^{n}\sum_{j=1}^{m}a_{i}b_{j}COV(Y_{i},X_{j}).\text{ \
(definition of covariance) }\square .
\end{eqnarray*}

\textit{Ask yourself:} observe that $COV(Y_{i},Y_{i})=VAR(Y_{i})$. \ Do you
see a link between statements (2) and (3) above?\bigskip

\subsection{The expected value and variance of the sample mean\protect\bigskip}

\begin{itemize}
\item Now consider \textit{independent} random variables $%
Y_{1},Y_{2},...Y_{n}$ that have the \textit{same }mean and the \textit{same }%
variance, that is:%
\begin{equation*}
E(Y_{i})=\mu \text{ and }VAR(Y_{i})=\sigma ^{2}\text{ }\forall \text{ }i.
\end{equation*}

If we define the \textbf{sample mean} as the statistic%
\begin{equation*}
\overline{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_{i},
\end{equation*}

then%
\begin{equation*}
E(\overline{Y})=\mu \text{ and }VAR(\overline{Y})=\frac{\sigma ^{2}}{n}.
\end{equation*}%
\newpage

Proof:

Note that $\overline{Y}$ is a linear function of the independent random
variables $Y_{1},Y_{2},...Y_{n}$ with all constants $a_{i}$ equal to $1/n.$
So:%
\begin{eqnarray*}
E(\overline{Y}) &=&E\left( \frac{1}{n}Y_{1}\right) +E\left( \frac{1}{n}%
Y_{2}\right) +...+E\left( \frac{1}{n}Y_{n}\right) \\
&=&\frac{1}{n}E(Y_{1})+\frac{1}{n}E(Y_{2})+...+\frac{1}{n}E(Y_{n}) \\
&=&\frac{1}{n}\mu +\frac{1}{n}\mu +...+\frac{1}{n}\mu \\
&=&\sum_{i=1}^{n}\frac{1}{n}\mu \\
&=&\mu \text{ \ \ }\square .
\end{eqnarray*}%
\bigskip

And:%
\begin{eqnarray*}
VAR(\overline{Y}) &=&\sum_{i=1}^{n}\left( \frac{1}{n}\right)
^{2}VAR(Y_{i})+2\sum_{i<j}\frac{1}{n}\frac{1}{n}COV(Y_{i},Y_{j})\text{ (from
above)} \\
&=&\sum_{i=1}^{n}\left( \frac{1}{n}\right) ^{2}VAR(Y_{i})+0\text{ \ \ \ \
(independence }\Rightarrow COV(Y_{i},Y_{j})=0\forall Y_{i},Y_{j}\text{)} \\
&=&\left( \frac{1}{n}\right) ^{2}\sum_{i=1}^{n}\sigma ^{2}\text{ (factoring
out constants, def. of variance)} \\
&=&\left( \frac{1}{n}\right) ^{2}n\sigma ^{2} \\
&=&\frac{\sigma ^{2}}{n}\text{ \ \ }\square .
\end{eqnarray*}
\end{itemize}

\end{document}
